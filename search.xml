<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>GFS论文阅读</title>
    <url>/2022/04/03/GFS/</url>
    <content><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>Google File System，一种专有分布式文件系统。GFS为在廉价商用设备上运行提供了容错能力，并可以在有大量客户端的情况下提供较高的整体性能。</p>
<span id="more"></span>

<p>分布式存储系统的四个背景：</p>
<ol>
<li>分布式组件经常发生错误，应当将此视为常态而不是意外</li>
<li>文件通常是大文件，而不是小文件，比如 GB 在这属于大文件，KB 级别属于小文件</li>
<li>大部分文件通过 append新数据的方式实现修改，而不是直接重写现有数据</li>
<li>同时设计应用程序和文件系统API便于提高整个系统的灵活性</li>
</ol>
<h2 id="系统假设"><a href="#系统假设" class="headerlink" title="系统假设"></a>系统假设</h2><ul>
<li>分布系统的各个组件是廉价的商品主机，而不是专业服务器，因此它们不得不频繁地自我监测、发现故障，并被要求有一定故障容错与自我故障恢复能力</li>
<li>文件数量处于几百万的规模，每一个文件的大小通常为 100 MB 或者更大，GB 也是很常见的数据大小；文件系统虽然支持小文件，但是不会进行特殊的优化（因此直接使用 GFS 应当基于大文件的应用背景）</li>
<li><strong>读工作负载</strong>主要由两种读方式构成：large streaming reads and small random reads.小规模的随机读取通常会在文件的某个任意偏移位置读几KB。性能敏感的应用程序通常会对小规模的随机读进行batch和sort处理，减少磁盘的来回读取</li>
<li><strong>写工作负载</strong>主要是大规模的、连续（即串行的）的写操作，这些操作将数据追加到文件末尾。写操作的规模通常和大规模串行读的规模类似。系统同样支持小规模随机写入，但不会做太多优化</li>
<li>系统需要支持并发写，即支持数百台机器并发地追加数据到一个文件。操作的原子性和同步开销是主要指标；</li>
<li><strong>高持续带宽（High sustained bandwidth）比低延迟更重要</strong></li>
</ul>
<h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><p>一个 GFS cluster（集群）分为两个组件：</p>
<ul>
<li><strong>单个</strong> master 节点；</li>
<li><strong>多个</strong> chunkserver 节点；</li>
</ul>
<p>一个 GFS 集群同时可以被多个 client（客户）节点访问。</p>
<p>一个 GFS 集群的架构可以用下图表示：</p>
<p><img src="/2022/04/03/GFS/1.png" alt="GFS Architecture"></p>
<p>可见 GFS 集群是一个典型 Master + Worker 结构。Master 来管理任务、分配任务，而 Worker 进行数据的存储和读取。</p>
<h3 id="分块存储"><a href="#分块存储" class="headerlink" title="分块存储"></a>分块存储</h3><p>分块可以进行并行操作，提升系统读写速度。<strong>GFS 以 64 MB 为固定的 Chunk Size 大小。</strong>远远大于典型的单机文件系统chunk的大小。分布式系统由于不可避免的故障，因此我们需要使用 replication 机制，每一个 chunk 都存在着若干个副本（它们不一定完全一样 ，因为 GFS 并不是一个强一致性文件管理系统），我们称这些 chunk 的副本为 replicas。每个 chunk 或者 replica 都作为普通的 Linux 文件存储在 chunkserver 上。</p>
<p>文件分块存储也引入了额外的复杂性：原本通过 file name 就能够定位文件，现在文件分为多个 chunk，每一个 chunk 甚至设计 replica，因此需要额外的信息和管理机制来确保文件分块存储的可行性；GFS 中每一个 chunk 会使用一个不可变的全局唯一（globally unique）64 位 chunk handle identity，这个标识由 Master 节点在 chunk 被创建时进行分配。</p>
<h3 id="master"><a href="#master" class="headerlink" title="master"></a>master</h3><p>Master 周期性通过 HeartBeat 机制和每一个 chunkserver 进行通信，进行指令的发送以及状态信息的接收</p>
<p>Master 节点<strong>在内存中存储</strong>着两个 Table，它们被统称为 metadata，它们存储的内容如下：</p>
<ul>
<li>Table 1：<ul>
<li>key：file name</li>
<li>value：<strong>an array of</strong> chunk handler (nv)</li>
</ul>
</li>
<li>Table 2：<ul>
<li>key：chunk handler</li>
<li>value：<ul>
<li><strong>a list of</strong> chunkserver(v)</li>
<li>chunk version number(nv)</li>
<li>which chunkserver is primary node(which means others are nomal chunkserver in the list)(v)</li>
<li>lease expiration time(v)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="client-读取"><a href="#client-读取" class="headerlink" title="client 读取"></a>client 读取</h3><ul>
<li>GFS Client 首先根据读取的 offset 在 chunk size 固定的背景下计算出 chunk index</li>
<li>给 GFS Master 发送 file name 以及 chunk index</li>
<li>GFS Master 接收到查询请求后，将 filename 以及 chunk index 映射为 chunk handle 以及 chunk locations，并返回给 GFS Client</li>
<li>GFS Client 接收到响应后以 key 为 file name + chunk index，value 为 chunk handle + chunk locations 的键值对形式缓存此次查询信息</li>
<li>接着，GFS Client 向其中一个 replicas (最有可能是最近的副本)发送请求，去请求中指定 chunk handle 以及块中的字节范围</li>
</ul>
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><p><strong>为什么设计这么大的chunk？</strong></p>
<ul>
<li>首先，它减少了 Client 与 Master 服务器交互的次数，因为对同一块进行多次读写仅仅需要向 Master 服务器发出一次初始请求，就能获取该chunk的所有位置信息。这可以有效地减少 Master 的工作负载。Client 在向 Master 索要 metadata 的部分数据之后，会对这部分数据进行超时的缓存，如果缓存未过期并且还是读写此块 chunk，那么 Client 就不需要向 Master 发出查询 metadata 的请求，直接使用缓存数据即可。如果chunk太小，一次读写可能会进行多次查询。</li>
<li>其次，减少了 GFS Client 与 GFS chunkserver 进行交互的数据开销，这是因为数据的读取具有连续读取的倾向，即读到 offset 的字节数据后，下一次读取有较大的概率读紧挨着 offset 数据的后续数据，chunk 的大尺寸相当于提供了一层缓存，减少了网络 I&#x2F;O 的开销</li>
<li>第三，它减少了存储在主服务器上的元数据的大小。这允许我们将元数据保存在内存中。</li>
<li>不过大chunk也有局限性，小数据量（比如仅仅占据一个 chunk 的文件，文件至少占据一个 chunk）的文件很多时，当很多 GFS Client 同时将 record 存储到该文件时就会造成局部的 hot spots 热点。不过GFS的应用场景是大文件</li>
</ul>
<p><strong>单master故障了怎么办？</strong></p>
<p>Master 提供 shadow Master 节点，这些节点在 Master 宕机时还能提供对文件系统的<strong>只读</strong>访问，但是注意，这里用词为 “shadow” 而不是 “mirror”，前者允许 shadow Master 的状态略微滞后于 Master 节点（通常延迟限制于几分之一秒内），后者要求必须保持一致。GFS 使用 shadow Master 节点在客户端不在意读到陈旧内容时能够很好的服务。不过大多数修改都是 append，因此客户端至多读不到新 append 的数据，而大概率不会读到错误的旧数据。</p>
<p>shadow Master 节点通过读取 Master 日志系统的 replicas 来进行状态的更新，然后根据日志的先后顺序执行操作，但是注意 shadow Master 节点并不参与和其他 chunkserver 进行通信，比如并不存在心跳机制。</p>
<p><strong>GFS设计为每个 chunk 都有两个replicas，对这三个chunk，如何保持一致性呢？</strong></p>
<p>对于 chunk 的写操作涉及两种修改：</p>
<ul>
<li>在相关 chunkserver 上进行 I&#x2F;O 写操作</li>
<li>在 Master 节点上修改 metadata</li>
</ul>
<p>系统通过lease机制来确保在 chunk server 的写入顺序：Master 节点将一个 lease 发给写操作涉及的三个 chunkserver 中的任意一个节点，此节点被称为 primary 节点，而其他两个节点被称为 secondaries(从属节点)。只有 primary 节点有权进行直接写入，且以其写入的顺序为准。</p>
<p>这种 Lease 机制减少了 Master 的管理开销，同时也确保了线程安全性，因为执行顺序不再由 Master 去决定，而是由拥有具体 lease 的 chunkserver 节点决定。租赁的默认占用超时时间为 60s，但是如果 Master 又接收到对同一个 chunk 的写操作，那么可以延长当前 primary 节点的剩余租赁时间。</p>
<ul>
<li>Step1：客户端向 Master 节点查询哪一个 Chunk Server 持有要进行写操作的 Chunk 的 Lease；</li>
<li>Step2：Master 节点回应 primary 节点的标识符（包括地址）以及其他 replicas 节点的地址。客户端接收后将此回应进行缓存，其会在 primary 节点不可达或者其不再持有 lease 时再次向 Master 查询；</li>
<li>Step3：客户端向所有的 replicas 都推送数据，注意此时客户端可以依靠任意顺序进行推送数据，并没有要求此时必须先给 primary 推送数据。所有的 chunkserver(replicas) 都会将推送来的数据存放在内置的 LRU buffer cacahe，缓存中的数据直到被使用或者超时才会被释放。</li>
<li>Step4：只要 replicas 回复已经接收到了所有数据，那么 Client 就会发送一个 write 指令给 primary 节点，primary 节点为多个写操作计划执行的序号（写操作可能来自于多个 Client），然后将此顺序应用于其本地 I&#x2F;O 写操作。</li>
<li>Step5：primary 节点将写操作请求转发给其他两个 replica，它们都将按照 primary 的顺序执行本地的 I&#x2F;O 写操作；</li>
<li>Step6：secondaries 从节点返回写成功的响应给 primary 节点；（secondaries节点写入失败，primary的数据依然会存在）</li>
<li>Step7：Primary 响应客户端，并返回该过程中发生的错误。注意，这里的错误不仅仅是 Primary 节点的写操作错误，还包括其他两个 replica 节点的写操作错误。如果 primary 自身发生错误，其就不会向其他两个 replica 节点进行转发。另一方面，如果 Client 收到写失败响应，那么其会重新进行写操作尝试，即重新开始 3-7 步。</li>
</ul>
<p><strong>master如何选择primary节点？</strong></p>
<ul>
<li>Master 节点在接受到修改请求时，会找此 file 文件最后一个 chunk 的 up-to-date 版本（最新版本），最新版本号应当等于 Master 节点维护的版本号。chunk 使用一个 chunk version 进行版本管理（分布式环境下普遍使用版本号进行管理，比如 Lamport 逻辑时钟）。一个修改涉及 3 个 chunk 的修改，如果某一个 chunk 因为网络原因没能够修改成功，那么其 chunk version 就会落后于其他两个 chunk，此 chunk 会被认为是过时的。Master通过心跳获取各个 chunkserver 拥有的 chunk 和其 chunk version</li>
<li>Master 在选择好 primary 节点后递增当前 chunk 的 chunk version，并通知 Primary，得到确切回复后再本地持久化</li>
<li>通过 Primary 与其他 chunkserver，发送修改此 chunk 的版本号的通知，而节点接收到此通知后会修改版本号，然后持久化</li>
<li>Primary 然后开始选择 file 最后一个文件的 chunk 的末尾 offset 开始写入数据，写入后将此消息转发给其他 chunkserver，它们也对相同的 chunk 在 offset 处写入数据</li>
</ul>
<h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p>更多内容请参考：</p>
<p><a href="https://spongecaptain.cool/post/paper/googlefilesystem/">一篇很详细的博客</a></p>
<p><a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/035fc972c796d33122033a0614bc94cff1527999.pdf">The Google File System 论文</a></p>
]]></content>
      <tags>
        <tag>分布式</tag>
        <tag>存储</tag>
      </tags>
  </entry>
  <entry>
    <title>Vim Memo List</title>
    <url>/2022/12/06/Vim%20Memo%20List/</url>
    <content><![CDATA[<p>…<span id="more"></span></p>
<h2 id="Normal"><a href="#Normal" class="headerlink" title="Normal"></a>Normal</h2><table>
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>G</td>
<td>移动到文件最后一行</td>
</tr>
<tr>
<td>gg</td>
<td>移动到文件第一行</td>
</tr>
<tr>
<td>n+Enter</td>
<td>向下移动 n 行</td>
</tr>
<tr>
<td>Ctrl-u &#x2F;Ctrl-d</td>
<td>上&#x2F;下半页</td>
</tr>
<tr>
<td>Ctrl-b&#x2F;Ctrl-f</td>
<td>向上&#x2F;向下翻页</td>
</tr>
<tr>
<td>b&#x2F;w</td>
<td>上一个&#x2F;下一个单词</td>
</tr>
<tr>
<td>ge&#x2F;e</td>
<td>上一个&#x2F;下一个词尾</td>
</tr>
<tr>
<td>0&#x2F;$</td>
<td>行的开始&#x2F;结束</td>
</tr>
<tr>
<td>^</td>
<td>行非空白开头</td>
</tr>
<tr>
<td>zt&#x2F;zz&#x2F;zb</td>
<td>当前行居上&#x2F;居中&#x2F;居下</td>
</tr>
<tr>
<td>&#x2F;word</td>
<td>向光标之下寻找一个名称为 word 的字串</td>
</tr>
<tr>
<td>n</td>
<td>重复前一个搜寻的动作</td>
</tr>
<tr>
<td>N</td>
<td>反向”进行前一个搜寻动作</td>
</tr>
<tr>
<td>:s&#x2F;word1&#x2F;word2&#x2F;g</td>
<td>在当前行寻找 word1 这个字串，并将该字串取代为 word2</td>
</tr>
<tr>
<td>:n1,n2s&#x2F;word1&#x2F;word2&#x2F;g</td>
<td>在第 n1 与 n2 行之间寻找word1 这个字串，并将该字串取代为 word2</td>
</tr>
<tr>
<td>:%s&#x2F;word1&#x2F;word2&#x2F;g</td>
<td>替换每一行的所有 word1 为 word2</td>
</tr>
<tr>
<td>:1,$s&#x2F;word1&#x2F;word2&#x2F;gc</td>
<td>在取代前显示提示字符给使用者确认 （confirm） 是否需要取代</td>
</tr>
<tr>
<td>(n) x, X</td>
<td>向后，前删除一(n)个字符</td>
</tr>
<tr>
<td>(n)dd</td>
<td>删除光标所在的那一(n)整行</td>
</tr>
<tr>
<td>(n)yy</td>
<td>复制光标所在的那一行</td>
</tr>
<tr>
<td>p, P</td>
<td>p 为将已复制的数据在光标下一行贴上，P 则为贴在光标上一行</td>
</tr>
<tr>
<td>u</td>
<td>复原前一个动作</td>
</tr>
<tr>
<td>Ctrl-r</td>
<td>重做上一个动作</td>
</tr>
<tr>
<td>.</td>
<td>重复前一个动作</td>
</tr>
<tr>
<td>:w [filename]</td>
<td>将编辑的数据储存成另一个文件</td>
</tr>
<tr>
<td>:r [filename]</td>
<td>将“filename” 这个文件内容加到光标所在列后面</td>
</tr>
<tr>
<td>:! command</td>
<td>暂时离开 vi 到命令行界面下执行 command 的显示结果</td>
</tr>
<tr>
<td>:set nu</td>
<td>显示行号</td>
</tr>
<tr>
<td>:set nonu</td>
<td>取消行号</td>
</tr>
</tbody></table>
<h2 id="Visual"><a href="#Visual" class="headerlink" title="Visual"></a>Visual</h2><table>
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>v</td>
<td>字符选择，会将光标经过的地方反白选择！</td>
</tr>
<tr>
<td>V</td>
<td>列选择，会将光标经过的列反白选择！</td>
</tr>
<tr>
<td>[Ctrl]+v</td>
<td>区块选择，可以用长方形的方式选择数据</td>
</tr>
<tr>
<td>y</td>
<td>将反白的地方复制起来</td>
</tr>
<tr>
<td>d</td>
<td>将反白的地方删除掉</td>
</tr>
<tr>
<td>p</td>
<td>将刚刚复制的区块，在光标所在处贴上！</td>
</tr>
</tbody></table>
<p>可以使用 vim 后面同时接好几个文件来同时打开</p>
<table>
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>:n</td>
<td>编辑下一个文件</td>
</tr>
<tr>
<td>:N</td>
<td>编辑上一个文件</td>
</tr>
<tr>
<td>:files</td>
<td>列出目前打开的所有文件</td>
</tr>
</tbody></table>
<h2 id="多窗口"><a href="#多窗口" class="headerlink" title="多窗口"></a>多窗口</h2><table>
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>:sp [filename]</td>
<td>打开一个新窗口，如果有加 filename， 表示在新窗口打开一个新文件，否则表示两个窗口为同一个文件内容（同步显示）。</td>
</tr>
<tr>
<td>[ctrl]+w  then ↑↓</td>
<td>光标移动到上（下）面的窗口</td>
</tr>
<tr>
<td>[ctrl]+w+q</td>
<td>结束当前窗口</td>
</tr>
</tbody></table>
<h2 id="vscode"><a href="#vscode" class="headerlink" title="vscode"></a>vscode</h2><table>
<thead>
<tr>
<th>Command</th>
<th>Keybinding</th>
</tr>
</thead>
<tbody><tr>
<td>Focus editor group</td>
<td>Ctrl + 数字</td>
</tr>
<tr>
<td>Toggle Terminal</td>
<td>Ctrl + &#96;</td>
</tr>
<tr>
<td>Go to File</td>
<td>Ctrl + p</td>
</tr>
<tr>
<td>split window</td>
<td>Ctrl + \</td>
</tr>
<tr>
<td>聚焦到工作目录</td>
<td>Ctrl+Shift+e</td>
</tr>
<tr>
<td>切换窗口</td>
<td>Ctrl + Tab 或者 Alt + 数字</td>
</tr>
</tbody></table>
]]></content>
      <tags>
        <tag>Vim</tag>
        <tag>VSCode</tag>
      </tags>
  </entry>
  <entry>
    <title>MapReduce论文阅读</title>
    <url>/2022/04/01/MapReduce/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>对于大数据量任务的处理，要想在可接受的时间内完成运算，只有将这些计算分布在成百上千的主机上。如何处理并行计算、如何分发数据、如何处理错误？所有这些问题综合在一起，需要大量的代码处理，因此也使得原本简单的运算变得难以处理。</p>
<p>为了解决上述复杂的问题，我们设计一个新的抽象模型，使用这个抽象模型，我们只要表述我们想要执行的简单运算即可，而不必关心并行计算、容错、数据分布、负载均衡等复杂的细节，这些问题都被封装在了一个库里面—MapReduce。</p>
<span id="more"></span>
<h2 id="Programming-Model"><a href="#Programming-Model" class="headerlink" title="Programming Model"></a>Programming Model</h2><p>MapReduce 库的用户只需要聚焦于两个函数的实现来进行任务的处理：Map 和 Reduce。</p>
<ul>
<li>Map, takes an input pair and produces a set of intermediate key&#x2F;value pairs. The MapReduce library groups together all intermediate values associated with the same intermediate key I and passes them to the Reduce function</li>
<li>The Reduce function, accepts an intermediate key I and a set of values for that key. It merges together these values to form a possibly smaller set of values. Typically just zero or one output value is produced per Reduce invocation. The intermediate values are supplied to the user’s reduce function via an iterator. This allows us to handle lists of values that are too large to fit in memory</li>
</ul>
<h2 id="Execution-Overview"><a href="#Execution-Overview" class="headerlink" title="Execution Overview"></a>Execution Overview</h2><p><img src="/2022/04/01/MapReduce/1.png" alt="Execution Overview"></p>
<ol>
<li>用户程序首先调用的 MapReduce 库将输入文件分成 M 个数据片段，每个数据片段的大小一般从 16MB 到 64MB（可以通过可选的参数来控制每个数据片段的大小）。然后用户程序在机群中创建大量的程序副本。</li>
<li>这些程序副本中的有一个特殊的程序–-master。副本中其它的程序都是 worker 程序，由 master 分配任务。有 M 个 Map 任务和 R 个 Reduce 任务将被分配，master 将一个 Map 任务或 Reduce 任务分配给一个空闲的 worker。</li>
<li>被分配了 map 任务的 worker 程序读取相关的输入数据片段，从输入的数据片段中解析出 key&#x2F;value pair，然后把 key&#x2F;value pair 传递给用户自定义的 Map 函数，由 Map 函数生成并输出的中间 key&#x2F;value pair，并缓存在内存中。</li>
<li>缓存中的 key&#x2F;value pair 通过分区函数分成 R 个区域，之后周期性的写入到本地磁盘上。缓存的 key&#x2F;value pair 在本地磁盘上的存储位置将被回传给 master，由 master 负责把这些存储位置再传送给 Reduce worker</li>
<li>当 Reduce worker 程序接收到 master 程序发来的数据存储位置信息后，使用 RPC 从 Map worker 所在主机的磁盘上读取这些缓存数据。当 Reduce worker 读取了所有的中间数据后，通过对 key 进行排序后使得具有相同 key 值的数据聚合在一起。由于许多不同的 key 值会映射到相同的 Reduce 任务上，因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。</li>
<li>Reduce worker 程序遍历排序后的中间数据，对于每一个唯一的中间 key 值，Reduce worker 程序将这个 key 值和它相关的中间 value 值的集合传递给用户自定义的 Reduce 函数。Reduce 函数的输出被追加到所属分区的输出文件。</li>
<li>当所有的 Map 和 Reduce 任务都完成之后，master 唤醒用户程序。在这个时候，在用户程序里的对 MapReduce 调用才返回。</li>
</ol>
<p>结束后会产生R个输出文件。用户可以将其feed到下一个MapReduce中去，或者在另外一个可以处理多个分割文件的分布式应用中使用。</p>
<h2 id="Master-Data-Structures"><a href="#Master-Data-Structures" class="headerlink" title="Master Data Structures"></a>Master Data Structures</h2><p>Master持有一些数据结构，它存储每一个Map和Reduce任务的状态(idle, in-progress, or completed), 以及Worker机器的标识(for non-idle tasks)。</p>
<p>Master就像一个数据管道，中间文件存储区域的位置信息通过这个管道从Map传递到Reduce。因此，对于每个已经完成的Map任务，master存储了Map任务产生的R个中间文件存储区域的大小和位置。当Map任务完成时，Master接收到位置和大小的更新信息，这些信息被逐步递增的推送给那些正在工作的Reduce任务。</p>
<h2 id="Fault-Tolerance"><a href="#Fault-Tolerance" class="headerlink" title="Fault Tolerance"></a>Fault Tolerance</h2><h3 id="Worker-Failure"><a href="#Worker-Failure" class="headerlink" title="Worker Failure"></a>Worker Failure</h3><p>master周期性的ping每个worker。如果在一个约定的时间范围内没有收到worker返回的信息，master将把这个worker标记为失效：</p>
<ul>
<li>由于 Map 任务将数据临时存储在本地，所有由这个失效的worker完成的或正在运行的Map任务被重设为初始的idle状态，之后这些任务再被安排给其他的worker。</li>
<li>对于这个woker的in-progress reduce task，同样重置为idle，而已经完成的Reduce任务的输出存储在全局文件系统上，因此不需要再次执行。</li>
</ul>
<p>当一个Map任务首先被worker A执行，之后由于worker A失效了又被调度到worker B执行，这个“重新执行”的动作会被通知给所有执行Reduce任务的worker。任何还没有从worker A读取数据的Reduce任务将从worker B读取数据。</p>
<h3 id="Master-Failure"><a href="#Master-Failure" class="headerlink" title="Master Failure"></a>Master Failure</h3><p>由于只有一个master进程，master失效后再恢复是比较麻烦的，因此我们现在的实现是如果master失效，就中止MapReduce运算。客户可以检查到这个状态，并且可以根据需要重新执行MapReduce操作。</p>
<h2 id="Locality"><a href="#Locality" class="headerlink" title="Locality"></a>Locality</h2><p>MapReduce的master在调度Map任务时会考虑输入文件的位置信息，尽量将一个Map任务调度在包含相关输入数据拷贝的机器上执行；如果上述努力失败了，master将尝试在保存有输入数据拷贝的机器附近的机器上执行Map任务(例如，分配到一个和包含输入数据的机器在一个switch里的worker机器上执行)。当在一个足够大的cluster集群上运行大型MapReduce操作的时候，大部分的输入数据都能从本地机器读取，因此消耗非常少的网络带宽。</p>
<h2 id="Backup-Tasks"><a href="#Backup-Tasks" class="headerlink" title="Backup Tasks"></a>Backup Tasks</h2><p>在运算过程中，如果有一台机器花了很长的时间才完成最后几个Map或Reduce任务，导致MapReduce操作总的执行时间超过预期。比如：一个机器的硬盘出了问题，在读取的时候要经常的进行读取纠错操作，导致读取数据的速度从30M&#x2F;s降低到1M&#x2F;s。比如cluster的调度系统在这台机器上又调度了其他的任务，由于CPU、内存、本地硬盘和网络带宽等竞争因素的存在，导致执行MapReduce代码的执行效率更加缓慢。</p>
<p>当一个MapReduce操作接近完成的时候，master调度备用（backup）任务进程来执行剩下的、处于处理中状态（in-progress）的任务。无论是最初的执行进程、还是备用（backup）任务进程完成了任务，我们都把这个任务标记成为已经完成。此机制通常只会占用比正常操作多几个百分点的计算资源。但能减少近 50% 的任务完成总时间。</p>
<h2 id="Refinements"><a href="#Refinements" class="headerlink" title="Refinements"></a>Refinements</h2><h3 id="Partitioning-Function"><a href="#Partitioning-Function" class="headerlink" title="Partitioning Function"></a>Partitioning Function</h3><p>MapReduce 缺省的分区函数是使用 hash 方法（比如，hash(key) mod R) 进行分区。hash 方法能产生非常平衡的分区。然而，有的时候，其它的一些分区函数对 key 值进行的分区将非常有用。比如，输出的 key 值是 URLs，有的用户希望每个主机的所有条目保持在同一个输出文件中。为了支持类似的情况，MapReduce 库的用户需要提供专门的分区函数。例如，使用“hash(Hostname(urlkey))mod R”作为分区函数就可以把所有来自同一个主机的 URLs 保存在同一个输出文件中。</p>
<h3 id="Ordering-Guarantees"><a href="#Ordering-Guarantees" class="headerlink" title="Ordering Guarantees"></a>Ordering Guarantees</h3><p>在给定的分区中，中间key&#x2F;value pair数据的处理顺序是按照key值增量顺序处理的。这样的顺序保证对每个分区成生成一个有序的输出文件，这对于需要对输出文件按key值随机存取的应用非常有意义，对在排序输出的数据集也很有帮助。</p>
<h3 id="Combiner-Function"><a href="#Combiner-Function" class="headerlink" title="Combiner Function"></a>Combiner Function</h3><p>在某些情况下，Map函数产生的中间key值的重复数据会占很大的比重，并且，用户自定义的Reduce函数满足结合律和交换律。比如一个单词统计程序。可以通过一个combiner函数首先在本地将这些记录进行一次合并，然后将合并的结果再通过网络发送出去。</p>
<p>Combiner函数在每台执行Map任务的机器上都会被执行一次。一般情况下，Combiner和Reduce函数是一样的。Combiner函数和Reduce函数之间唯一的区别是MapReduce库怎样控制函数的输出。Reduce函数的输出被保存在最终的输出文件里，而Combiner函数的输出被写到中间文件里，然后被发送给Reduce任务。</p>
<h3 id="Side-effects"><a href="#Side-effects" class="headerlink" title="Side-effects"></a>Side-effects</h3><p>通常应用程序首先把输出结果写到一个临时文件中，在输出全部数据之后，在使用系统级的原子操作 rename 重新命名这个临时文件。</p>
<h3 id="Skipping-Bad-Records"><a href="#Skipping-Bad-Records" class="headerlink" title="Skipping Bad Records"></a>Skipping Bad Records</h3><p>每个worker进程都设置了信号处理函数捕获内存段异常（segmentation violation）和总线错误（bus error）。在执行Map或者Reduce操作之前，MapReduce库通过全局变量保存记录序号。如果用户程序触发了一个系统信号，消息处理函数将用“最后一口气”通过UDP包向master发送处理的最后一条记录的序号。当master看到在处理某条特定记录不止失败一次时，master就标志着条记录需要被跳过，并且在下次重新执行相关的Map或者Reduce任务的时候跳过这条记录。(前提是忽略一些有问题的记录也是可以接受的)</p>
<h3 id="Counters"><a href="#Counters" class="headerlink" title="Counters"></a>Counters</h3><p>用户可能想统计已经处理了多少个单词、已经索引的多少篇German文档等等。用户在程序中创建一个命名的计数器对象，在Map和Reduce函数中相应的增加计数器的值。这些计数器的值周期性的从各个单独的worker机器上传递给master（附加在ping的应答包中传递）。master把执行成功的Map和Reduce任务的计数器值进行累计，当MapReduce操作完成之后，返回给用户代码。</p>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>MapReduce编程模型在Google内部成功应用于多个领域。这种成功可以归结为几个方面：</p>
<ul>
<li>MapReduce封装了并行处理、容错处理、数据本地化优化、负载均衡等等技术难点的细节，这使得MapReduce库易于使用</li>
<li>大量不同类型的问题都可以通过MapReduce简单的解决。比如，MapReduce用于生成Google的网络搜索服务所需要的数据、用来排序、用来数据挖掘、用于机器学习，以及很多其它的系统</li>
<li>实现了一个在数千台计算机组成的大型集群上灵活部署运行的MapReduce。这个实现使得有效利用这些丰富的计算资源变得非常简单，因此也适合用来解决其他很多需要大量计算的问题。</li>
</ul>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul>
<li>分布式的Grep：Map函数输出匹配某个模式的一行，Reduce函数是一个恒等函数，即把中间数据复制到输出。</li>
<li>计算URL访问频率：Map函数处理日志中web页面请求的记录，然后输出(URL,1)。Reduce函数把相同URL的value值都累加起来，产生(URL,记录总数)结果。</li>
<li>倒转网络链接图：Map函数在源页面（source）中搜索所有的链接目标（target）并输出为(target,source)。Reduce函数把给定链接目标（target）的链接组合成一个列表，输出(target,list(source))。</li>
<li>每个主机的检索词向量：检索词向量用一个(词,频率)列表来概述出现在文档或文档集中的最重要的一些词。Map函数为每一个输入文档输出(主机名,检索词向量)，其中主机名来自文档的URL。Reduce函数接收给定主机的所有文档的检索词向量，并把这些检索词向量加在一起，丢弃掉低频的检索词，输出一个最终的(主机名,检索词向量)。</li>
<li>倒排索引：Map函数分析每个文档输出一个(词,文档号)的列表，Reduce函数的输入是一个给定词的所有（词，文档号），排序所有的文档号，输出(词,list（文档号）)。所有的输出集合形成一个简单的倒排索引，它以一种简单的算法跟踪词在文档中的位置。</li>
<li>分布式排序：Map函数从每个记录提取key，输出(key,record)。Reduce函数不改变任何的值。这个运算依赖分区机制(在4.1描述)和排序属性(在4.2描述)。</li>
</ul>
<h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf">MapReduce: Simplified Data Processing on Large Clusters</a></p>
]]></content>
      <tags>
        <tag>分布式</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Bitcoin: A Peer-to-Peer Electronic Cash System</title>
    <url>/2022/11/22/bitcoin/</url>
    <content><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution.</p>
<p>We propose a solution to the double-spending problem using a peer-to-peer network.</p>
<span id="more"></span>
<p>The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work.</p>
<p>As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they’ll generate the longest chain and outpace attackers.</p>
<h2 id="Transactions"><a href="#Transactions" class="headerlink" title="Transactions"></a>Transactions</h2><p>Each owner transfers the coin to the next by digitally signing a hash of the previous transaction and the public key of the next owner and adding these to the end of the coin.<br><img src="/2022/11/22/bitcoin/pic1.png" alt="pic1"></p>
<p>Problem &#x2F; Solving</p>
<ul>
<li>outright forgery  （signing ）</li>
<li>double spending （public log）</li>
<li>theft</li>
</ul>
<p>The problem of course is the payee can’t verify that one of the owners did not double-spend the coin.<br>A transaction does not take effect when it is written to a block by a miner, but only after the chain has actually become the longest chain on the chain before the transaction is considered truly irreversible. This is why it is recommended to wait for six confirmations after each transfer. Each new block added by the exchange after the block is a confirmation, and after waiting for six confirmations, most miners recognize the chain as the longest chain and the transaction will be irreversible.</p>
<p>Since it is not possible to apply a consensus algorithm like raft, which makes decisions based on the majority, to a decentralized system.</p>
<h2 id="Proof-of-Work"><a href="#Proof-of-Work" class="headerlink" title="Proof-of-Work"></a>Proof-of-Work</h2><p>The proof-of-work involves scanning for a value that when hashed, such as with SHA-256, the hash begins with a number of zero bits. The average work required is exponential in the number of zero bits required and can be verified by executing a single hash.</p>
<p>there is a mount of work a node needs to do to acturally be able to extend the log</p>
<ul>
<li>winner in Pow decides on next log entry</li>
<li>hard to impersonate winner</li>
</ul>
<p><img src="/2022/11/22/bitcoin/pic2.png" alt="pic2.png"></p>
<h2 id="forks"><a href="#forks" class="headerlink" title="forks"></a>forks</h2><p>if</p>
<ol>
<li>find nonce at same time </li>
<li>slow network<br>rule: peer switches to longest fork</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>比特币白皮书（Bitcoin: A Peer-to-Peer Electronic Cash System）<br>稍微详细的论文阅读笔记可以参考 <a href="https://www.cnblogs.com/xinzhao/p/8584477.html">精读比特币论文</a></p>
]]></content>
      <tags>
        <tag>分布式</tag>
        <tag>区块链</tag>
        <tag>比特币</tag>
      </tags>
  </entry>
  <entry>
    <title>Paxos算法介绍</title>
    <url>/2022/04/07/paxos/</url>
    <content><![CDATA[<p>在该一致性算法中，有三种参与角色，Proposer、Acceptor 和 Learner</p>
<span id="more"></span>

<p><img src="/2022/04/07/paxos/1.png" alt="算法流程"></p>
<h2 id="Prepare阶段"><a href="#Prepare阶段" class="headerlink" title="Prepare阶段"></a>Prepare阶段</h2><p>Proposer 选择一个新的提案编号Mn（所有的编号是全序的），然后 Acceptor 的某个超过半数的子集成员发送编号为 Mn 的 Prepare 请求，要求该集合中的 Acceptor 做出如下回应：</p>
<ul>
<li>向 Proposer 承诺，保证不再批准任何编号小于 Mn 的提案</li>
<li>如果 Acceptor 已经批准过任何提案，那么其就向 Proposer 反馈当前该 Acceptor 已经批准的编号的提案的值</li>
</ul>
<p>每个 Acceptor只需要记住它已经做出 Prepare 请求响应的提案的最大编号或者已经批准的提案的最大编号和值，以便在出现故障或者节点重启的情况下，也能保证P2c的不变性。</p>
<p>对于 Proposer 来说，只要它可以保证不会产生具有相同编号的提案，那么就可以丢弃任意的提案以及它的所有运行时状态信息。</p>
<h2 id="Accept阶段"><a href="#Accept阶段" class="headerlink" title="Accept阶段"></a>Accept阶段</h2><p>如果 Proposer 收到来自半数以上的 Acceptor 对于其发出的编号为 Mn 的 Prepare 请求的响应，那么它就会发送一个针对【Mn，Vn】提案的 Accept 请求给 Acceptor。</p>
<p>如果 Acceptor 收到这个针对【Mn，Vn】提案的 Accept 请求，只要该 Acceptor 尚未对编号大于 Mn 的 Prepare 请求做出过保证，它就可以通过这个提案</p>
<h2 id="提案获取"><a href="#提案获取" class="headerlink" title="提案获取"></a>提案获取</h2><p>让 Learn 获取到已经被半数以上的 Acceptor 批准的提案：</p>
<ul>
<li>一个简单的做法是，一旦 Acceptor 批准了一个提案，就将该提案发送给所有的 Learner。这种做法可以让 Learner 尽快地获取被选定的提案，但是需要让每个 Acceptor 与所有的 Learner 逐个进行一次通信…</li>
<li>另一种可行方案是，将结果发送给主 Learner，通过 Learner之间的通信来互相感知提案的选定，这种方案减少了通信次数，但会存在单点故障的问题</li>
<li>一个折中的方案是，Acceptor 将批准的提案发送给一个特定的 Learner 集合，集合中 Learner 的个数越多，可靠性越好，但同时网络通信的复杂度越高</li>
</ul>
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><p><strong>我们的目的是选取一个最终的一致性值 V，为什么需要引入提案编号 M 进行辅助呢？</strong></p>
<p>如果不使用编号，Acceptor 在接受到提案后就进行批准的情况下，没办法选出大多数。因此引入一个全局唯一的序号进行关联，且生成的编号是全序的，不过编号不再算法的考虑之内。</p>
<p><strong>为什么算法要求至少保证 Prepare 请求和 Accept 请求都发送给半数以上的 Acceptor 子集？</strong></p>
<p>任意两个半数以上的集合，必定包含至少一个公共元素，简单来说，就说不会形成一个孤岛，独自通过一个议案</p>
<p><strong>算法一定有效吗？</strong></p>
<p>Basic Paxos 算法很大程度上保证了有效性，但依然会存在这样一种可能：</p>
<p><img src="/2022/04/07/paxos/2.png" alt="活锁"></p>
<p>两个 Proposer 交替 Prepare 成功，而 Accept 失败，形成活锁（Livelock）。</p>
<p>为了这种情况，就必须选择一个主 Proposer，并规定只有主 Proposer 才能提出议案，这样没有 Proposer 竞争，解决了活锁问题。在系统中仅有一个 Proposer 进行 Value 提交的情况下，Prepare阶段就可以跳过，从而将两阶段变为一阶段，提高效率。</p>
<p>选取一个主 Proposer 也是一次决议的过程，可以通过 Basic paxos 算法选取。</p>
<p><strong>选取主 Proposer 不是也会出现上述问题吗？</strong></p>
<p>我的理解是，选取主 Proposer 的过程可以看作是算法 init 的阶段，失败了重试就可以了，并不会对数据造成影响，只要保证后续的可持续性就OK</p>
]]></content>
      <tags>
        <tag>分布式</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>二进制炸弹</title>
    <url>/2020/11/01/BinaryBomb/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近上系统级编程的课，其中一个实验是破解二进制炸弹，下面记录一下解题思路。</p>
<span id="more"></span>

<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>由于老师提供了binarybomb移植版文件，包含：</p>
<ul>
<li>gdb.exe</li>
<li>objdump.exe</li>
</ul>
<p>一个调试工具一个反汇编工具，但这是Windows的移植版。在Linux下使用对应的gdb和objdump即可</p>
<h3 id="反汇编"><a href="#反汇编" class="headerlink" title="反汇编"></a>反汇编</h3><p>在bomb.exe文件下路径下，执行<strong>objdump -d bomb.exe</strong>，得到了程序的汇编代码。然后大致浏览了一下，发现其中有六个函数phase_1……phase_6，基本上也就可以确定就是这六个关卡了。</p>
<h2 id="phase-1"><a href="#phase-1" class="headerlink" title="phase_1"></a>phase_1</h2><p><strong>知识点：string，函数调用，栈</strong></p>
<p>找到phase_1对应的汇编代码，可以看到就那么10几行</p>
<p><img src="/2020/11/01/BinaryBomb/phase1.png" alt="phase1"></p>
<p>不难发现，其中一个call指令，调用了string_not_equal函数。可以猜测到应该就是<strong>比较我们输入的字符串密码和内置的密码是否相等</strong></p>
<p>接下来使用gdb调试程序，这个函数前三行都是给函数分配栈空间，对实验没什么影响，我们在第四行设置一个断点，运行：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">b *0x40184c  <span class="comment">#设置断点</span></span><br><span class="line">r     <span class="comment">#运行</span></span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/01/BinaryBomb/r.png" alt="r"></p>
<p>然后乱输入一个123456789进入，那么我们输入的字符串被放在哪里了呢？肯定是作为一个参数要传进字符串比较这个函数的，才可以进行比较。在调用这个函数之前，是把这个eax寄存器中的值传给了esp的，我们不妨查看一下eax中的值：</p>
<p><img src="/2020/11/01/BinaryBomb/eax.png" alt="eax"></p>
<p>发现就是我们的输入，那要和哪个字符串进行比较呢？肯定也是最为参数传递进去了的，在第四行代码中，将一个地址的值放在了esp+4的位置，不出意外密码应该就是这个地址上的值了，接下来打印一下：</p>
<p><img src="/2020/11/01/BinaryBomb/1%E5%AF%86%E7%A0%81.png" alt="1密码"></p>
<p>密码就是<code>Public speaking is very easy.</code>.&#x2F;BinaryBomb<br>接下来重新运行程序，输入密码就进入了第二关：</p>
<p><img src="/2020/11/01/BinaryBomb/phase2.png" alt="phase2"></p>
<h2 id="phase-2"><a href="#phase-2" class="headerlink" title="phase_2"></a>phase_2</h2><p><strong>知识点：循环</strong></p>
<p><img src="/2020/11/01/BinaryBomb/phase2as.png" alt="phase2as"></p>
<p>首先，大致看一下汇编代码，可以很显然的看到箭头处调用了一个函数：<strong>read_six_numbers</strong>。说明第二关是要输入6个数字。在调用了这个函数之后，把ebp-0x24中的值赋给eax寄存器，如果不等于1，则爆炸！那么我们可以猜测第一个数字就应该是1，为了验证这个想法我们在这一行之后设置断点，然后随便输入6个数字。这里我就输入5 2 0 1 3 14了。</p>
<p><img src="/2020/11/01/BinaryBomb/eax2.png" alt="eax2"></p>
<p>然后查看此时eax中的值是多少：</p>
<p><img src="/2020/11/01/BinaryBomb/%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%95%B0%E5%AD%97.png" alt="第一个数字"></p>
<p>果然，里面的值就是我们输入的第一个数字。所以第一个数字是1不会爆炸。</p>
<p>知道这一点后我们继续往下看代码，标有数字2的代码段，给[ebp-c]处的位置加1后如果值小于等于5就跳转到401896那一行。很显然，这是一个循环。5次循环，每一次得到后一个数字，接下来查找这里面的规律。</p>
<p>在2的上方，如果比较eax和edx不相等就爆炸，再往上看到3这个位置 $edx &#x3D; ($edx+1)*$eax。最终得到的edx中的值就应该是我们希望知道的数字，edx加1之前的值就是循环计数的值，所以5次循环中，加1后依次是2 3 4 5 6.还要乘以eax中的值，那此时eax中的值是什么呢？通过1-3之间的代码不难看出就是存放的前一个数字，如果推断不出来，通过gdb查看也是可以的。总结一下这五个数字的关系</p>
<p>N<sub>k</sub> &#x3D; (n+1) N<sub>k-1</sub>  （N<sub>k</sub>表示第k个数字，n表示第几次循环）</p>
<p>由于第一个数字是1，带入上方公式，第二个数字就是2×1&#x3D;2，第三个数字就是3×2&#x3D;6…依次是24、120、720。至此，第二关通过！</p>
<h2 id="Phase-3"><a href="#Phase-3" class="headerlink" title="Phase_3"></a>Phase_3</h2><p><strong>知识点：switch语句</strong></p>
<p>首先分析一下前面一段汇编代码：</p>
<p><img src="/2020/11/01/BinaryBomb/3-0.png" alt="3-0"></p>
<p>在标号1处，修改esp上方区域的值，用<code>lea</code>指令传递地址（这里传递了三个变量，可以猜测是否就是需要输入三个变量呢？），这是再向后一个调用的函数传递参数。可以看到后面调用了<code>sscan()</code>函数。查了一下这个函数的声明如下：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">sscanf</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *str, <span class="type">const</span> <span class="type">char</span> *format, ...)</span></span><br></pre></td></tr></table></figure>

<p>接着看到箭头处，我们先来查一下这个地址里装的什么：</p>
<p><img src="/2020/11/01/BinaryBomb/%E5%8F%82%E6%95%B0.png" alt="参数"></p>
<p>显然，是<code>sscan()</code>的format参数。这是要我们输入三个变量：<strong>整形 字符型 整形</strong>。</p>
<p>在标号2的代码段中，如果函数返回值（正确输入的个数）不大于2就爆炸。</p>
<p>在标号1处 -0x10(%ebp)、-0x11(%ebp)、-0x18(%ebp)处就存放了我们的三个输入，接着往下看：</p>
<p><img src="/2020/11/01/BinaryBomb/switch.png" alt="switch"></p>
<p>把第一个输入的数字和7比较，如果大于7就跳转（紧接着爆炸，其实就是到default），ja是无符号比较。也就是说我们可以确定第一个整数的范围就是<strong>0-7</strong>。</p>
<p>所以最后跳转的地方根据eax不同有8中情况。这应该就是switch语句了，调试查询处这几个跳转的地址：</p>
<p><img src="/2020/11/01/BinaryBomb/%E8%B7%B3%E8%BD%AC.png" alt="跳转"></p>
<p>接着往下看代码，根据这个地址分成了8个部分：</p>
<p><img src="/2020/11/01/BinaryBomb/switch2.png" alt="switch2"></p>
<p>也就是说这一关的答案不是唯一的，下面我们就拿2号开始尝试。</p>
<p>把0x62存入到-0x9(%ebp)中，把我们输入的第三个整数存入到eax中和0x2f3比较，不相等就爆炸，所以如果第一个整数是2那第三个整数应该是755（十进制）,接着jump:</p>
<p><img src="/2020/11/01/BinaryBomb/%E5%AD%97%E7%AC%A6.png" alt="字符"></p>
<p>到了这个函数的最后，可以得知0x62就是第二个输入，0x62对应的是字符b。</p>
<p>所以其中一种可能的输入是 <strong>2 b 755</strong>。其他的不再赘述，可以自行尝试，至此，第三关通过！</p>
]]></content>
      <tags>
        <tag>cs</tag>
        <tag>gdb</tag>
      </tags>
  </entry>
  <entry>
    <title>Raft论文阅读</title>
    <url>/2022/04/12/raft/</url>
    <content><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><ul>
<li>Strong Leader：比如日志只从 Leader 发送给其他服务器。这种方式简化了对复制日志的管理并且使得 Raft 算法更加易于理解。</li>
<li>Leader election：使用随机计时器来选举领导人，解决冲突时会更加简单快捷<span id="more"></span></li>
</ul>
<h2 id="复制状态机"><a href="#复制状态机" class="headerlink" title="复制状态机"></a>复制状态机</h2><p><img src="/2022/04/12/raft/pic1.png" alt="状态机"></p>
<p>复制状态机通常都是基于复制日志实现的，如上图。每一个服务器存储一个包含一系列指令的日志，并且按照日志的顺序进行执行。每一个日志都按照相同的顺序包含相同的指令，所以每一个服务器都执行相同的指令序列。因为每个状态机都是确定的，每一次执行操作都产生相同的状态和同样的序列。</p>
<p><strong>一致性算法的任务是保证复制日志的一致性。</strong>服务器上的一致性模块接收客户端发送的指令然后添加到自己的日志中。它和其他服务器上的一致性模块进行通信来保证每一个服务器上的日志最终都以相同的顺序包含相同的请求，即使有些服务器发生故障。一旦指令被正确的复制，每一个服务器的状态机按照日志顺序处理他们，然后输出结果被返回给客户端。因此，服务器集群看起来形成了一个高可靠的状态机。</p>
<h2 id="Paxos算法的问题"><a href="#Paxos算法的问题" class="headerlink" title="Paxos算法的问题"></a>Paxos算法的问题</h2><ul>
<li>晦涩难懂</li>
<li>单决策，还没有一种被广泛认同的多决策问题的算法</li>
<li>不易于构建实际系统：Paxos uses a symmetric peer-to-peer approach at its core (though it eventually suggests a weak form of leadership as a performance optimization).</li>
</ul>
<h2 id="Raft设计目标"><a href="#Raft设计目标" class="headerlink" title="Raft设计目标"></a>Raft设计目标</h2><ul>
<li>必须提供一个完整的基础以供实现实际的系统</li>
<li>必须在任何情况下都是安全的</li>
<li>在大多数的情况下都是可用的</li>
<li>它的大部分操作必须是高效的</li>
<li>最重要也是最大的挑战是可理解性</li>
</ul>
<h2 id="Raft-一致性算法"><a href="#Raft-一致性算法" class="headerlink" title="Raft 一致性算法"></a>Raft 一致性算法</h2><p>Raft 将一致性问题分解成了三个相对独立的子问题：</p>
<ul>
<li><strong>领导选举</strong>：当现存的 Leader 发生故障的时候，一个新的 Leader 需要被选举出来（章节 5.2）</li>
<li><strong>日志复制</strong>：Leader 必须从 Client 接收日志条目（log entries）然后复制到集群中的其他节点，并强制要求其他节点的日志和自己保持一致。</li>
<li><strong>安全性</strong>：在 Raft 中安全性的关键是在图 3 中展示的状态机安全：如果有任何的服务器节点已经应用了一个确定的日志条目到它的状态机中，那么其他服务器节点不能在同一个日志索引位置应用一个不同的指令。章节 5.4 阐述了 Raft 算法是如何保证这个特性的；这个解决方案涉及到选举机制（5.2 节）上的一个额外限制。</li>
</ul>
<h3 id="Raft-基础"><a href="#Raft-基础" class="headerlink" title="Raft 基础"></a>Raft 基础</h3><ul>
<li>Leader ：处理所有 Client 的请求</li>
<li>Follower ：不会发送任何请求，只是简单地响应来自 Leader 和 candidates 的请求</li>
<li>Candidate ：在选举新 Leader 时使用</li>
</ul>
<p><img src="/2022/04/12/raft/pic2.png" alt="pic2"></p>
<p>Raft 把时间分成任意长度的 term，用连续的整数标记，每一段 term 从一次选举开始， Raft 保证了在一个给定的任期内，最多只有一个 Leader（Candidate 赢得选举后行使 Leader 的职责）。term 在 Raft 算法中充当逻辑时钟的作用。</p>
<p><img src="/2022/04/12/raft/pic3.png" alt="pic3"></p>
<p>每当服务器之间通信的时候都会交换当前 term#；如果一个服务器的当前 term#比其他服务器小，那么他会更新自己的编号到较大的编号值。如果一个 candidate 或者 Leader 发现自己的 term# 过期了，那么他会立即恢复成 Follower 状态。如果一个节点接收到一个包含过期的 term# 的请求，那么他会直接拒绝这个请求。</p>
<p>服务器节点之间通过RPC进行交流，基本的算法只需要两种类型的RPC：</p>
<ul>
<li>RequestVote：由 Candidates 在选举期间发起</li>
<li>AppendEntries：由 Leader 发起，用来复制日志和提供一种心跳机制（carry no log entries）</li>
</ul>
<h3 id="Leader-election"><a href="#Leader-election" class="headerlink" title="Leader election"></a>Leader election</h3><p>当服务器启动时，都是处于 Follower 状态。如果一个 Follower 在一段时间（election timeout）没有收到任何消息，会认为系统中没有可用的 Leader，从而触发一次选举。</p>
<p>开启一次选举时， Follower 增加当前 term# 并切换到 Candidate 状态，直到以下情况发生：</p>
<ul>
<li>Candidate 收到了 majority 的投票，赢得了此次选举，转变为 Leader 状态，并向其他 Server 发送心跳，通告自己的 Leader 身份<ul>
<li>Each server will vote for at most one candidate in a given term, on a first-come-first-served basis</li>
</ul>
</li>
<li>在等待投票结果的时候，此 Candidate 可能会收到其他 server 的 claim（heartbeat with term#），如果这个 Leader 的 term# 不小于此 Candidate 当前的 term#，那么 Candidate 会承认 Leader 合法并回到 Follower State</li>
<li>Candidate 可能既没有赢得选举也没有输：如果有多个 Follower 同时成为Candidate，那么选票可能会被瓜分以至于没有 Candidate 可以赢得 majority 的支持。当这种情况发生的时候，每一个 Candidate 都会超时，然后通过增加当前 term# 来开始一轮新的选举</li>
</ul>
<p>Raft 算法使用随机 election timeouts (e.g., 150–300ms)。的方法来确保很少会发生选票瓜分的情况，同样的，每一个 Candidate 在开始一次选举的时候会重置一个随机的选举超时时间，然后在超时时间内等待投票的结果。使用随机化来简化 Raft 中 Leader 选举算法（相比Paxos而言）</p>
<h3 id="Log-replication"><a href="#Log-replication" class="headerlink" title="Log replication"></a>Log replication</h3><p>一旦一个 Leader 被选举出来，他就开始为 Client 提供服务。Client 的每一个请求都包含一条被复制状态机执行的指令。领导人把这条指令作为一条新的 log entry 附加到日志中去，然后并行的发起 AppendEntries RPCs 给其他的服务器，让他们复制这条 log entry。当此 log entry 已经写入大多数的 Server， Leader 就会执行此指令，然后返回结果给 Client。Leader 会不断重试，直到确保所有的 Follower 都存储了此 log entry。</p>
<p><img src="/2022/04/12/raft/pic4.png" alt="pic4"></p>
<p>如上图所示。每一个 log entry 存储一条状态机指令和从领导人收到这条指令时的term#。日志中的 term# 用来检查是否出现不一致的情况。每一个 log entry 同时也都有一个整数索引值来表明它在日志中的位置。领导人跟踪了最大的将会被提交的日志项的索引，并且索引值会被包含在未来的所有 AppendEntries RPCs（包括心跳包）。一旦 Follower 知道一条 log entry 已经被提交（Leader 保证其会应用到状态机中），那么他也会将这个 log entry 应用到本地的状态机中（按照日志的顺序）。</p>
<p>日志匹配特性：</p>
<ul>
<li>对于一个给定的 term#，最多只会有一个 Leader 被选举出来</li>
<li>Leader 绝对不会删除或者覆盖自己的日志，只会增加</li>
<li>如果两个日志在某一相同索引位置 log entry 的 term# 相同，那么我们就认为这两个日志从头到该索引位置之间的内容完全一致<ul>
<li>在发送 AppendEntries RPC 的时候，Leader 会把新的 log entry 前紧挨着的条目的索引位置和任期号包含在日志内。如果 Follower 在它的日志中找不到包含相同 index 和 term# 的 entry，那么他就会拒绝接收此 entry</li>
</ul>
</li>
<li>如果某个 log entry 在某个 term# 中已经被提交，那么这个 entry 必然出现在更大 term# 的所有 Leader 中</li>
<li>如果某一服务器已将给定索引位置的日志条目应用至其状态机中，则其他任何服务器在该索引位置不会应用不同的日志条目</li>
</ul>
<p><img src="/2022/04/12/raft/pic5.png" alt="pic5"></p>
<p>如上图所示，Leader 崩溃的情况下会使得日志处于不一致的状态，因为老的 Leader 可能还没有把log entry 复制到其他 Server 就挂掉了。</p>
<p>在 Raft 算法中，Leader 是通过强制 Followers 直接复制自己的日志来处理不一致问题的（Leader 从来不会覆盖或者删除自己的日志）。Leader 必须找到最后两者达成一致的地方，然后删除 Follower 从那个点之后的所有 log entry，并发送自己在那个点之后的日志给跟 Follower（添加或覆盖）。</p>
<p>Leader 针对每一个 Follower 都维护了一个 nextIndex（初始化为 Leader 的最后一个 entry 的 index + 1），表示下一个需要发送给 Follower 的那个 entry 的索引，如果 Follower 的日志和 Leader 不一致，那么在下一次的 AppendEntries RPC 时的一致性检查就会失败。在被 Follower 拒绝之后，领导人就会减小 nextIndex 值并进行重试。最终会达成一致。</p>
<h3 id="Safety"><a href="#Safety" class="headerlink" title="Safety"></a>Safety</h3><p>到目前为止描述的机制并不能充分的保证每一个状态机会按照相同的顺序执行相同的指令。例如一个 Follower 可能会进入不可用状态，但 Leader 已经提交了若干日志，整个 Follower 如果之后被选举为 Leader，会导致不同的状态机执行了不同的指令序列，所以下面还要增加一个选举的限制：</p>
<p>Candidate 为了赢得选举必须联系集群中的大部分节点，这意味着每一个已经提交的 log entry 在这些服务器节点中肯定存在于至少一个节点上。（两个 majority 至少有一个公共元素）。RequestVote RPC 实现了这样的限制：RPC 中包含了 Candidate 的日志信息，然后投票人会拒绝掉那些日志没有自己新（更大的 term# 或者 更多的 entry）的投票请求。</p>
<p>Leader 可能把日志复制到了大多数服务器上，而在提交之前崩溃了，如下图：</p>
<p><img src="/2022/04/12/raft/pic6.png" alt="pic6"></p>
<p>导致一条已经被存储到大多数节点上的老日志条目，也依然有可能会被未来的 Leader 覆盖掉 （c → d）。（d，e 是 c 后的两种不同的情况）</p>
<p>为了消除上图里描述的情况，在提交之前 term 的日志项时，必须保证当前 term 新建的日志项已经复制到超过半数节点。这样，之前 term 的日志项才算真正提交的。如上图 e 所示，通过 4 的提交，间接保证了 2 的提交，不会出现日志被后续leader覆盖的情况</p>
<h3 id="Follower和Candidate崩溃"><a href="#Follower和Candidate崩溃" class="headerlink" title="Follower和Candidate崩溃"></a>Follower和Candidate崩溃</h3><p>如果 Follower 或者 Candidate 崩溃了，那么后续发送给他们的 RPCs 都会失败。Raft 中处理这种失败就是简单的通过无限的重试；如果崩溃的机器重启了，那么这些 RPC 就会完整的成功。</p>
<h3 id="Timing-and-availability"><a href="#Timing-and-availability" class="headerlink" title="Timing and availability"></a>Timing and availability</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">broadcastTime ≪ electionTimeout ≪ MTBF <span class="comment">//平均故障间隔时间</span></span><br></pre></td></tr></table></figure>

<ul>
<li>广播时间必须比选举超时时间小一个量级，这样 Leader 才能够发送稳定的心跳消息来阻止 Followers 开始进入选举状态</li>
<li>选举超时时间应该要比平均故障间隔时间小上几个数量级，这样整个系统才能稳定的运行。</li>
</ul>
<p>广播时间和平均故障间隔时间是由系统决定的，但是选举超时时间是我们自己选择的</p>
<h3 id="Log-compaction"><a href="#Log-compaction" class="headerlink" title="Log compaction"></a><strong>Log compaction</strong></h3><ul>
<li>Each server takes snapshots independently, covering just the committed entries in its log</li>
<li>Raft also includes a small amount of metadata in the snapshot: the <em>last included index</em> is the index of the last entry in the log that the snapshot replaces (the last entry the state machine had applied), and the <em>last included term</em> is the term of this entry.</li>
<li>When a follower receives a snapshot with this RPC<ul>
<li>Usually the snapshot will contain new information not already in the recipient’s log. In this case, the follower discards its entire log</li>
<li>If instead the follower receives a snapshot that describes a prefix of its log (due to retransmission or by mistake), then log entries covered by the snapshot are deleted but entries following the snapshot are still valid and must be retained.</li>
</ul>
</li>
<li>Servers must decide when to snapshot. One simple strategy is to take a snapshot when the log reaches a fixed size in bytes.</li>
<li>The operating system’s copy-on-write support (e.g., fork on Linux) can be used to create an in-memory snapshot of the entire state machine</li>
</ul>
<h3 id="Client-interaction"><a href="#Client-interaction" class="headerlink" title="Client interaction"></a><strong>Client interaction</strong></h3><ul>
<li>if the leader crashes after committing the log entry but before responding to the client, the client will retry the command with a new leader, causing it to be executed a second time.<ul>
<li>The solution is for clients to assign unique serial numbers to every command. Then, the state machine tracks the latest serial number processed for each client, along with the associated response. If it receives a command whose serial number has already been executed, it responds immediately without re-executing the request.</li>
</ul>
</li>
</ul>
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><p>如何维护 term#</p>
<p>考虑以下几种情况：</p>
<ul>
<li>leader 已经提交，但还没有通知其他server，这个时候挂了</li>
<li>leader 发送 AE RPC 后挂了，已经复制到了大多数server上</li>
<li>leader 提交后挂了，但还没有在通知应用层执行</li>
</ul>
<p><strong>Suppose a new leader is elected while the network is partitioned, but the old leader is in a different partition. How will the old leader know to stop committing new entries?</strong></p>
<p>The old leader will either not be able to get a majority of successful responses to its AppendEntries RPCs (if it’s in a minority partition), or if it can talk to a majority, that majority must overlap with the new leader’s majority, and the servers in the overlap will tell the old leader that there’s a higher term. That will cause the old leader to switch to follower.</p>
<p><strong>What if the election timeout is too short? Will that cause Raft to malfunction?</strong></p>
<p>A bad choice of election timeout does not affect safety, it onlyaffects liveness. If the election timeout is too small, then followers may repeatedly time out before the leader has a chance to send out any AppendEntries. In that case Raft may spend all its time electing new leaders, and no time processing client requests. If the election timeout is too large, then there will be a needlessly large pause after a leader failure before a new leader is elected.</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>英文<a href="https://ramcloud.atlassian.net/wiki/download/attachments/6586375/raft.pdf">论文地址</a></p>
<p>中文<a href="https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md">翻译地址</a></p>
<p>Raft<a href="http://thesecretlivesofdata.com/raft/">动画演示</a></p>
]]></content>
      <tags>
        <tag>分布式</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Mit6.824 lab3 实现思路</title>
    <url>/2022/12/24/mit824lab3/</url>
    <content><![CDATA[<h2 id="Part-A-Key-x2F-value-service-without-snapshots"><a href="#Part-A-Key-x2F-value-service-without-snapshots" class="headerlink" title="Part A: Key&#x2F;value service without snapshots"></a><strong>Part A: Key&#x2F;value service without snapshots</strong></h2><blockquote>
<p>The kvserver code submits the Put&#x2F;Append&#x2F;Get operation to Raft, so that the Raft log holds a sequence of Put&#x2F;Append&#x2F;Get operations. All of the kvservers execute operations from the Raft log in order, applying the operations to their key&#x2F;value databases.</p>
</blockquote>
<span id="more"></span>
<blockquote>
<p>Your first task is to implement a solution that works when there are no dropped messages, and no failed servers.</p>
</blockquote>
<p>先实现一个在网络条件正常情况下能顺利运行的简易版本：</p>
<h3 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h3><p>客户端要发起 Put&#x2F;Append&#x2F;Get RPC 请求：</p>
<p>向作为 Leader 的 server 发起请求， 所以需要维护一个 LeaderId</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Clerk <span class="keyword">struct</span> &#123;</span><br><span class="line">	...</span><br><span class="line">	leaderId <span class="type">int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">MakeClerk</span><span class="params">(servers []*labrpc.ClientEnd)</span></span> *Clerk &#123;</span><br><span class="line">	...</span><br><span class="line">	ck.leaderId = <span class="number">0</span>      </span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>发起Get请求，当请求的服务器不是 Leader 的时候，修改 LeaderId，再次请求，客户端对错误的处理是重试，Put&#x2F;Append请求同理</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ck *Clerk)</span></span> Get(key <span class="type">string</span>) <span class="type">string</span> &#123;</span><br><span class="line">	args := GetArgs&#123;</span><br><span class="line">		ClientId:    ck.clientId,</span><br><span class="line">		OperationId: ck.operationId,</span><br><span class="line">		Key:         key,</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		requestId := nrand()</span><br><span class="line">		reply := GetReply&#123;&#125;</span><br><span class="line">		DPrintf(<span class="string">&quot;[requestId %v]Sending Get RPC to [%v]: &#123;args %v&#125;&quot;</span>, requestId, ck.leaderId, args)</span><br><span class="line">		ok := ck.servers[ck.leaderId].Call(<span class="string">&quot;KVServer.Get&quot;</span>, &amp;args, &amp;reply)</span><br><span class="line">		DPrintf(<span class="string">&quot;[requestId %v]Get RPC to [%v]: &#123;reply %v&#125; &#123;ok %v&#125;&quot;</span>, requestId, ck.leaderId, reply, ok)</span><br><span class="line">		<span class="keyword">if</span> !ok || reply.Err == ErrWrongLeader &#123;</span><br><span class="line">			ck.leaderId = (ck.leaderId + <span class="number">1</span>) % <span class="built_in">len</span>(ck.servers)</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		&#125;</span><br><span class="line">		ck.operationId++</span><br><span class="line">		<span class="keyword">if</span> reply.Err == ErrNoKey &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> reply.Value</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a>服务端</h3><p>对收到的客户端发来的请求操作，将要执行的操作封装在<code>Op</code>中，传给 Raft 层的 <code>Start()</code> 函数，达成共识：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Op <span class="keyword">struct</span> &#123;</span><br><span class="line">	Type  <span class="type">string</span></span><br><span class="line">	Key   <span class="type">string</span></span><br><span class="line">	Value <span class="type">string</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其次，需要一个存储数据的地方，实验中简单使用一个map进行kv的存储：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> KVServer <span class="keyword">struct</span> &#123;</span><br><span class="line">	...</span><br><span class="line">	kvStore <span class="keyword">map</span>[<span class="type">string</span>]<span class="type">string</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">StartKVServer</span><span class="params">(servers []*labrpc.ClientEnd, me <span class="type">int</span>, persister *raft.Persister, maxraftstate <span class="type">int</span>)</span></span> *KVServer &#123;</span><br><span class="line">	...</span><br><span class="line">	kv.kvStore = <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="type">string</span>]<span class="type">string</span>)</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>RPC handler 收到请求后，需要等待 raft 层 commit 该条日志，也就是 server 收到 applyCh 上的信息后，将该操作 apply 到 <code>kvStore</code> 中，再响应客户端。</p>
<p>所以 server 需要一直监听 applyCh 上的信息，可以使用一个协程去完成此工作。</p>
<p>另外，当收到 applyCh 上的信息后，要通知 handler 去响应对应发起请求的客户端。如何做到这一点呢？ 由于 start() 会返回该日志的 index，一个实现方案是创建一个 <code>notifyChs map[int]chan</code> ，以 index 作为键，创建一个 chan 进行等待，当收到 applyCh 上的信息后，去<code>notifyChs</code>中找到对应的 chan 进行通知。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> KVServer <span class="keyword">struct</span> &#123;</span><br><span class="line">	...</span><br><span class="line">	notifyChs  <span class="keyword">map</span>[<span class="type">int</span>]<span class="keyword">chan</span> OpContext</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">StartKVServer</span><span class="params">(servers []*labrpc.ClientEnd, me <span class="type">int</span>, persister *raft.Persister, maxraftstate <span class="type">int</span>)</span></span> *KVServer &#123;</span><br><span class="line">	...</span><br><span class="line">	kv.notifyChs = <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="type">int</span>]<span class="keyword">chan</span> OpContext)</span><br><span class="line">	<span class="keyword">go</span> kv.applier()</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>至此，可以通过第一个测试：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">la4am12@Workstatio:~/mit6.824/lab/src/kvraft$ go <span class="built_in">test</span> -run TestBasic3A -race</span><br><span class="line">Test: one client (3A) ...</span><br><span class="line">  ... Passed --  15.1  5  7733 1262</span><br><span class="line">PASS</span><br><span class="line">ok      6.824/kvraft    15.122s</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Now you should modify your solution to continue in the face of network and server failures.</p>
</blockquote>
<p>接下来考虑异常情况下服务的可靠性，可能会面临如下问题：</p>
<ul>
<li>client 和 server 间网络异常，例如，server 收到 client 的指令并执行，由于某种原因，client 没有收到这个 server 的回复，client 会再次发起这个命令，导致一个操作在 server 上 apply 多次，最终结果异常。</li>
<li>server 之间网络异常，例如调用 Start() 后，raft 对该日志并没有达成共识，当在 applyCh 中收到该 index 的日志被提交时，可能并不是起初的那条日志了。因为 raft 会把没有达成共识的日志覆盖掉。</li>
</ul>
<p>对于第一个问题，要保证 linearizable，每一个相同的请求，只能够执行一次。所以，server 需要能够发现 resend 的请求，并直接返回原本的结果，而不是继续执行。</p>
<p>论文中给出的可行方案是 server 为每个 client 维护一个 session，以 clientId + operationId 唯一区分一个请求，同时记录每个请求的回复。考虑这样一种场景， client 向 leader 服务器发起请求，commit 之后，leader crush，此时 client 选择下一个 leader 重新发起请求，此时虽然是不同的服务器收到该请求，同样不能执行第二次。所以，所有的服务器必须都能够区分出 resend 的请求。</p>
<h3 id="客户端修改"><a href="#客户端修改" class="headerlink" title="客户端修改"></a>客户端修改</h3><p>初始化一个随机数作为 clientId，同时以一个递增的数作为 operationId，client 在没有处理好一个请求的时候不会进行下一个请求，所以使用递增的 id 可以让 server 清理小于该 id 的所有 reply 记录。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Clerk <span class="keyword">struct</span> &#123;</span><br><span class="line">	...</span><br><span class="line">	clientId    <span class="type">int64</span></span><br><span class="line">	operationId <span class="type">int64</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">MakeClerk</span><span class="params">(servers []*labrpc.ClientEnd)</span></span> *Clerk &#123;</span><br><span class="line">	...</span><br><span class="line">	operationId = <span class="number">0</span></span><br><span class="line">	ck.clientId = nrand()</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在请求参数中携带上 id 信息：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> PutAppendArgs <span class="keyword">struct</span> &#123;</span><br><span class="line">	...</span><br><span class="line">	ClientId    <span class="type">int64</span></span><br><span class="line">	OperationId <span class="type">int64</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> GetArgs <span class="keyword">struct</span> &#123;</span><br><span class="line">	...</span><br><span class="line">	ClientId    <span class="type">int64</span></span><br><span class="line">	OperationId <span class="type">int64</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>client 在收到成功的响应后递增 operationId 。</p>
<h3 id="服务端修改"><a href="#服务端修改" class="headerlink" title="服务端修改"></a>服务端修改</h3><p>使用一个 map 去记录每个 client 的上一个 operation 的信息：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> KVServer <span class="keyword">struct</span> &#123;</span><br><span class="line">	...</span><br><span class="line">	lastOpMemo <span class="keyword">map</span>[<span class="type">int64</span>]OpContext</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> OpContext <span class="keyword">struct</span> &#123;</span><br><span class="line">	ClientId    <span class="type">int64</span></span><br><span class="line">	OperationId <span class="type">int64</span></span><br><span class="line">	Reply       Reply</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Reply <span class="keyword">struct</span> &#123;</span><br><span class="line">	Err   Err</span><br><span class="line">	Value <span class="type">string</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>以 PutAppend 请求为例，server 收到请求后，要检查是否为 duplicated request，当收到 notifyCh 的信息后，要检查是否为原本的 operation，应对上述描述的第二个问题。</p>
<p>每个请求需要进行超时等待。例如可能会出现这种情况：假设所有 client 向一个 server 发起了请求后，server crush，此时日志没有复制到大多数节点上，该 server 恢复后不再是 leader，那么如果没有 client 继续提交请求，该 index 处的 chan 始终不会有结果，处于 livelock 状态，所以需要等待一段时间（设置的500ms）后返回超时错误给client。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *KVServer)</span></span> PutAppend(args *PutAppendArgs, reply *PutAppendReply) &#123;</span><br><span class="line">	op := Op&#123;</span><br><span class="line">		ClientId:    args.ClientId,</span><br><span class="line">		OperationId: args.OperationId,</span><br><span class="line">		Type:        args.Op,</span><br><span class="line">		Key:         args.Key,</span><br><span class="line">		Value:       args.Value,</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	kv.mu.RLock()</span><br><span class="line">	<span class="keyword">if</span> lastOp, ok := kv.isDuplicated(op); ok &#123;</span><br><span class="line">		DPrintf(<span class="string">&quot;server %v received duplicated %v request for clientId %v OperationId %v&quot;</span>, kv.me, args.Op, args.ClientId, args.OperationId)</span><br><span class="line">		reply.Err = lastOp.Reply.Err</span><br><span class="line">		kv.mu.RUnlock()</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line">	kv.mu.RUnlock()</span><br><span class="line"></span><br><span class="line">	index, _, isLeader := kv.rf.Start(op)</span><br><span class="line">	<span class="keyword">if</span> !isLeader &#123;</span><br><span class="line">		reply.Err = ErrWrongLeader</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	notifyCh := <span class="built_in">make</span>(<span class="keyword">chan</span> OpContext, <span class="number">1</span>)</span><br><span class="line">	kv.mu.Lock()</span><br><span class="line">	kv.notifyChs[index] = notifyCh</span><br><span class="line">	kv.mu.Unlock()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">select</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> opContext := &lt;-notifyCh:</span><br><span class="line">		<span class="keyword">if</span> opContext.ClientId != args.ClientId || opContext.OperationId != args.OperationId &#123;</span><br><span class="line">			reply.Err = ErrWrongLeader</span><br><span class="line">			<span class="keyword">return</span></span><br><span class="line">		&#125;</span><br><span class="line">		reply.Err = opContext.Reply.Err</span><br><span class="line">	<span class="keyword">case</span> &lt;-time.After(time.Duration(Timeout) * time.Millisecond):</span><br><span class="line">		reply.Err = ErrTimeout</span><br><span class="line">		DPrintf(<span class="string">&quot;PutAppend timeout : server %v clientId %v operationId %v&quot;</span>, kv.me, args.ClientId, args.OperationId)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		kv.mu.Lock()</span><br><span class="line">		<span class="built_in">delete</span>(kv.notifyChs, index)</span><br><span class="line">		kv.mu.Unlock()</span><br><span class="line">	&#125;()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在收到 applyCh 的信息后，也需要验证是否为重复命令。同时，在 apply 之后，要记录这个操作在 <code>lastOpMemo</code>中（这里直接进行覆盖，因为 client 在没有处理好一个请求时，不会进行下一个请求，似乎没有问题）：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *KVServer)</span></span> applier() &#123;</span><br><span class="line">	<span class="keyword">for</span> !kv.killed() &#123;</span><br><span class="line">		applyMsg := &lt;-kv.applyCh</span><br><span class="line">		kv.mu.Lock()</span><br><span class="line">		<span class="keyword">if</span> applyMsg.CommandValid &#123;</span><br><span class="line">			<span class="keyword">if</span> applyMsg.CommandIndex &lt;= kv.lastApplied &#123;</span><br><span class="line">				kv.mu.Unlock()</span><br><span class="line">				<span class="keyword">continue</span></span><br><span class="line">			&#125;</span><br><span class="line">			op := applyMsg.Command.(Op)</span><br><span class="line">			<span class="keyword">var</span> opContext OpContext</span><br><span class="line">			<span class="keyword">if</span> lastOp, ok := kv.isDuplicated(op); ok &#123;</span><br><span class="line">				opContext = lastOp</span><br><span class="line">			&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">				opContext = kv.applyOp(op)</span><br><span class="line">				kv.lastOpMemo[op.ClientId] = opContext</span><br><span class="line">				kv.lastApplied = applyMsg.CommandIndex</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">if</span> notifyCh, ok := kv.notifyChs[applyMsg.CommandIndex]; ok &#123;</span><br><span class="line">				notifyCh &lt;- opContext</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">if</span> kv.needSnapshot() &#123;</span><br><span class="line">				kv.takeSnapshot(applyMsg.CommandIndex)</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; </span><br><span class="line">		kv.mu.Unlock()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>至此可以通过 lab 3A 测试：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">la4am12@Workstatio:~/mit6.824/lab/src/kvraft$ go <span class="built_in">test</span> -run 3A -race</span><br><span class="line">Test: one client (3A) ...</span><br><span class="line">  ... Passed --  15.1  5  7308 1199</span><br><span class="line">Test: ops complete fast enough (3A) ...</span><br><span class="line">  ... Passed --  10.1  3  4248    0</span><br><span class="line">Test: many clients (3A) ...</span><br><span class="line">  ... Passed --  15.5  5  7936 1711</span><br><span class="line">Test: unreliable net, many clients (3A) ...</span><br><span class="line">  ... Passed --  15.9  5  6027 1198</span><br><span class="line">Test: concurrent append to same key, unreliable (3A) ...</span><br><span class="line">  ... Passed --   1.3  3   216   52</span><br><span class="line">Test: progress <span class="keyword">in</span> majority (3A) ...</span><br><span class="line">  ... Passed --   0.4  5    42    2</span><br><span class="line">Test: no progress <span class="keyword">in</span> minority (3A) ...</span><br><span class="line">  ... Passed --   1.0  5   201    3</span><br><span class="line">Test: completion after heal (3A) ...</span><br><span class="line">  ... Passed --   1.0  5    58    3</span><br><span class="line">Test: partitions, one client (3A) ...</span><br><span class="line">  ... Passed --  22.1  5  7018 1170</span><br><span class="line">Test: partitions, many clients (3A) ...</span><br><span class="line">  ... Passed --  23.2  5 10787 1649</span><br><span class="line">Test: restarts, one client (3A) ...</span><br><span class="line">  ... Passed --  19.0  5 10060 1256</span><br><span class="line">Test: restarts, many clients (3A) ...</span><br><span class="line">  ... Passed --  19.6  5 16398 1706</span><br><span class="line">Test: unreliable net, restarts, many clients (3A) ...</span><br><span class="line">  ... Passed --  20.8  5  7043 1168</span><br><span class="line">Test: restarts, partitions, many clients (3A) ...</span><br><span class="line">  ... Passed --  26.8  5 17833 1524</span><br><span class="line">Test: unreliable net, restarts, partitions, many clients (3A) ...</span><br><span class="line">  ... Passed --  27.1  5  6445  920</span><br><span class="line">Test: unreliable net, restarts, partitions, random keys, many clients (3A) ...</span><br><span class="line">  ... Passed --  29.9  7 12051 1339</span><br><span class="line">PASS</span><br><span class="line">ok      6.824/kvraft    249.296s</span><br></pre></td></tr></table></figure>

<h2 id="Part-B-Key-x2F-value-service-with-snapshots"><a href="#Part-B-Key-x2F-value-service-with-snapshots" class="headerlink" title="Part B: Key&#x2F;value service with snapshots"></a><strong>Part B: Key&#x2F;value service with snapshots</strong></h2><blockquote>
<p>As things stand now, your key&#x2F;value server doesn’t call your Raft library’s Snapshot() method, so a rebooting server has to replay the complete persisted Raft log in order to restore its state. Now you’ll modify kvserver to cooperate with Raft to save log space, and reduce restart time, using Raft’s Snapshot() from Lab 2D.</p>
</blockquote>
<blockquote>
<p>Think about when a kvserver should snapshot its state and what should be included in the snapshot.</p>
</blockquote>
<p>当命令commit后，也就是 server 收到 applyCh 消息的时候，去判断 <code>persister.RaftStateSize()</code> 和阈值<code>maxraftstate</code>的大小，决定是否进行snapshot。快照必须包括用以保证 linearizable 语义的元素：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *KVServer)</span></span> takeSnapshot(index <span class="type">int</span>) &#123;</span><br><span class="line">	w := <span class="built_in">new</span>(bytes.Buffer)</span><br><span class="line">	e := labgob.NewEncoder(w)</span><br><span class="line">	e.Encode(kv.kvStore)</span><br><span class="line">	e.Encode(kv.lastOpMemo)</span><br><span class="line">	e.Encode(kv.lastApplied)</span><br><span class="line"></span><br><span class="line">	kv.rf.Snapshot(index, w.Bytes())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>考虑 restart 的情况，需要利用 snapshot 进行还原。另外，当 log cannot catch up 的时候，raft 会使用 InstallSnapshot Rpc 进行同步，server 在 applyCh 中会收到 snapshot，需要进行apply。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *KVServer)</span></span> applier() &#123;</span><br><span class="line">	<span class="keyword">for</span> !kv.killed() &#123;</span><br><span class="line">		applyMsg := &lt;-kv.applyCh</span><br><span class="line">		kv.mu.Lock()</span><br><span class="line">		<span class="keyword">if</span> applyMsg.CommandValid &#123;</span><br><span class="line">    ...</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> applyMsg.SnapshotValid &#123;</span><br><span class="line">			<span class="keyword">if</span> applyMsg.SnapshotIndex &lt;= kv.lastApplied &#123;</span><br><span class="line">				kv.mu.Unlock()</span><br><span class="line">				<span class="keyword">continue</span></span><br><span class="line">			&#125;</span><br><span class="line">			kv.applySnapshots(applyMsg.Snapshot)</span><br><span class="line">		&#125;</span><br><span class="line">		kv.mu.Unlock()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>course lab</tag>
        <tag>raft</tag>
      </tags>
  </entry>
</search>
